#!/bin/bash


#--CONFIGURE VARIABLES--
env_script="${1:-/dev/null}"
slurm_conf_path=/etc/slurmify/slurm/
worker_script=/usr/share/slurmify/build_worker.sh
mkdir -p /var/log/slurmify/
log_file=/var/log/slurmify/node00.log

config_file=/etc/slurmify/slurmify.conf
if [ ! -f "$config_file" ]; then
    echo "Error: $config_file not found."
    exit 1
fi

# Use 3 and 4 to refer to terminal stdout and stderr
exec 3>&1 4>&2
# Redirect default output to log file
exec >> $log_file 2>&1

# Parse config file
nodes=()
current_section=""
while IFS= read -r line || [[ -n "$line" ]]; do
    # 1) Skip empty lines or lines starting with '#' (comments)
    if [[ -z "$line" || $line =~ ^[[:space:]]*# ]]; then
        continue
    fi

    # 2) Detect section headers: [something]
    if [[ $line =~ ^\[(.+)\]$ ]]; then
        current_section=${BASH_REMATCH[1]}

        # If this section name starts with "node", we will create an associative array for it
        if [[ $current_section =~ ^node[0-9]+$ ]]; then
            nodes+=( "$current_section" )
            declare -gA "$current_section"
        fi

    # 3) Detect key-value pairs: key=value
    elif [[ $line =~ ^([_[:alpha:]][_[:alnum:]]*)=(.*)$ ]]; then
        var_name=${BASH_REMATCH[1]}
        var_value=${BASH_REMATCH[2]}

        if [[ $current_section =~ ^node[0-9]+$ ]]; then
            declare ${current_section}[$var_name]="${var_value}"

        else
            new_var="${current_section}_${var_name}"
            declare -g "${new_var}=${var_value}"
        fi
    fi

done < "$config_file"
#---------------------

#-----MOUNT DRIVE----- TODO: support more fs types
echo Mounting shared filesystem... >&3
mount_script=/usr/share/slurmify/mounts/$filesystem_type.sh
mount_args="$filesystem_mount \
$filesystem_addr \
$filesystem_usr \
$filesystem_pwd"
source $mount_script $mount_args
#---------------------

echo Configuring master node... >&3

#-EDIT SLURM CONFIGS--
slurm_nodes=""
gres_nodes=""
for node in "${nodes[@]}"; do
    name="${node}[name]"
    addr="${node}[addr]"
    cpus="${node}[cpus]"
    gpus="${node}[gpus]"
    slurm_nodes+="NodeName=${!name} NodeAddr=${!addr} CPUs=${!cpus} Gres=gpu:${!gpus} State=UNKNOWN\n"
    if ! [ ${!gpus} = 0 ]; then 
        gres_nodes+="NodeName=${!name} Name=gpu File=/dev/nvidia"
        gres_ids="0\n"
        if ! [ ${!gpus} = 1 ]; then 
            gres_ids="[0-$(($gpus - 1))]\n"
        fi
        gres_nodes+=$gres_ids
    fi        
done

sed -i "s/NodeName= NodeAddr= CPUs= Gres= State=UNKNOWN/$slurm_nodes/" ${slurm_conf_path}slurm.conf
sed -i s/ClusterName=/ClusterName=${cluster_name}/ ${slurm_conf_path}slurm.conf 

master_addr="${node00[addr]}"
sed -i "s/SlurmctldHost=/SlurmctldHost=node00($master_addr)/" ${slurm_conf_path}slurm.conf

# Configure workers partition (if enough nodes present).
# Note: this relies on strict naming pattern: node00 node01 node02 ...
last_index=$(( ${#nodes[@]} - 1 )) 
if ((last_index>0)); then
    if ((last_index==1)); then 
        worker_nodes="node01"
    elif ((last_index>1)); then
        worker_nodes="node[01-$(printf "%02d" $last_index)]"
    fi
    echo "PartitionName=workers Nodes=$worker_nodes Default=YES MaxTime=INFINITE State=UP" >> ${slurm_conf_path}slurm.conf
else 
    sed -i "s/Default=NO/Default=YES/" ${slurm_conf_path}slurm.conf
fi

sed -i "s@NodeName= Name=gpu File=/dev/nvidia0@$gres_nodes@" ${slurm_conf_path}gres.conf
echo ${filesystem_mount}* >> ${slurm_conf_path}cgroup_allowed_devices_file.conf
#---------------------

#---UPDATE HOSTNAME--- 

hostname node00  

hosts=""
for node in "${nodes[@]}"; do
    name="${node}[name]"
    addr="${node}[addr]"
    hosts+="${!addr} ${!name}\n"
done

# Docker doesn't like you directly "sed-ing" `hosts`, here's a hacky work around:
mkdir /tmp/slurmify
temp_hosts=$(mktemp /tmp/slurmify/hosts.new.XXXXXX)
temp_hostname=$(mktemp /tmp/slurmify/hostname.new.XXXXXX)

cp /etc/hosts $temp_hosts
sed -i 2d $temp_hosts
sed -i "2i $hosts" $temp_hosts
cp -f $temp_hosts /etc/hosts

cp /etc/hostname $temp_hostname
sed -i 's/.*/node00/' $temp_hostname
cp -f $temp_hostname /etc/hostname

sed -i 's/^preserve_hostname: false$/preserve_hostname: true/' /etc/cloud/cloud.cfg
#---------------------

#------NTPDATE--------
NEEDRESTART_MODE=l apt-get -o DPkg::Lock::Timeout=60 install ntpdate -y
#---------------------

#-------SLURM---------
NEEDRESTART_MODE=l apt-get -o DPkg::Lock::Timeout=60 install slurm-wlm -y
mkdir -p $filesystem_mount/slurm/
cp -r "${slurm_conf_path}"* /etc/slurm/
cp -r "${slurm_conf_path}"* $filesystem_mount/slurm/
cp /etc/munge/munge.key $filesystem_mount
service munge start
service slurmd start
service slurmctld start
#---------------------

#----GNU PARALLEL-----
apt-get -o DPkg::Lock::Timeout=60 -y install parallel
#---------------------

#--BUILD WORKERS--
touch ~/.ssh/known_hosts
chmod 600 ~/.ssh/known_hosts

run_on_node() {
    local node=$1
    local args=( $node "$hosts" $filesystem_mount $env_script )

    log_file=/var/log/slurmify/$node.log

    if ! [ $node = 'node00' ]; then
        if ! ssh-keygen -F $node; then
            ssh-keyscan -t ed25519 -H $node >> ~/.ssh/known_hosts
        fi
        ssh -i ~/.ssh/$key_name $node "bash -s -- $mount_args" < $mount_script > $log_file 2>&1
        ssh -i ~/.ssh/$key_name $node "bash -s -- ${args[@]@Q}" < $worker_script >> $log_file 2>&1
    else        
        source $env_script $filesystem_mount
    fi
}

# Export the function and vars to make them available to parallel
export -f run_on_node
export worker_script hosts filesystem_mount mount_script mount_args env_script key_name

# Run worker_script in parallel on all nodes
echo Configuring worker nodes... >&3
parallel -j 0 run_on_node {} ::: "${nodes[@]}"
#---------------------